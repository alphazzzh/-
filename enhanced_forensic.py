import logging
import json
import re
import hashlib
import pickle
import asyncio
from typing import TypedDict, Optional, Dict, Any, Tuple, List
from pathlib import Path

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import BaseOutputParser
from langgraph.graph import StateGraph, END

# ============= æ–°å¢ç¦»çº¿éªŒè¯å·¥å…·å¯¼å…¥ =============
try:
    from rbloom import Bloom
except ImportError:
    try:
        from pybloom_live import BloomFilter as Bloom
    except ImportError:
        Bloom = None

try:
    import spacy
    try:
        # ä¼˜åŒ–: ç¦ç”¨ä¸éœ€è¦çš„ç»„ä»¶ï¼Œåªä¿ç•™ NER (ç”¨äºå®ä½“è¯†åˆ«)
        # æ˜¾å¼æ·»åŠ  sentencizer ç”¨äºåˆ†å¥
        nlp_en = spacy.load("en_core_web_sm", disable=['tagger', 'parser', 'attribute_ruler', 'lemmatizer'])
        nlp_en.add_pipe('sentencizer')
    except:
        nlp_en = None
    try:
        # ä¸­æ–‡åŒç†
        nlp_zh = spacy.load("zh_core_web_sm", disable=['tagger', 'parser', 'attribute_ruler', 'lemmatizer'])
        nlp_zh.add_pipe('sentencizer')
    except:
        nlp_zh = None
except ImportError:
    spacy = None
    nlp_en = None
    nlp_zh = None

try:
    import stdnum.isbn
    import stdnum.issn
    HAS_STDNUM = True
except ImportError:
    HAS_STDNUM = False

try:
    from gibberish_detector import detector as gibberish_detector
    HAS_GIBBERISH = True
except ImportError:
    HAS_GIBBERISH = False

# Optional: NLI (ONNX)
try:
    import numpy as np
    import onnxruntime as ort
    from transformers import AutoTokenizer
except Exception:
    np = None
    ort = None
    AutoTokenizer = None

# å¯¼å…¥sentencetransformer
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    HAS_EMBEDDING = True
except ImportError:
    HAS_EMBEDDING = False
    SentenceTransformer = None
    np = None


# ================= [æ–°å¢æ¨¡å—] ç¦»çº¿æ‹¦æˆªå™¨ä¸ NLTK è¡¥ä¸ =================

# 1. å¼ºåˆ¶ç¦»çº¿ç¯å¢ƒå˜é‡
import os
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["CUDA_VISIBLE_DEVICES"] = "6"

# 2. PyTorch ç‰ˆæœ¬ä¼ªè£…
import torch
torch.__version__ = "2.6.0"

# 3. Transformers è·¯å¾„æ‹¦æˆª
from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer
from config import AppConfig, LOCAL_SAFE_PATH

orig_config_load = AutoConfig.from_pretrained
orig_model_load = AutoModelForSequenceClassification.from_pretrained
orig_tok_load = AutoTokenizer.from_pretrained

def is_target_model(path):
    return any(k in str(path) for k in [
        "roberta-large", "MiniCheck", "lytang",
        "Bespoke", "bespokelabs"
    ])

def mocked_config_load(cls, path, *args, **kwargs):
    if is_target_model(path): kwargs['local_files_only'] = True
    return orig_config_load.__func__(cls, LOCAL_SAFE_PATH if is_target_model(path) else path, *args, **kwargs)

def mocked_model_load(cls, path, *args, **kwargs):
    if is_target_model(path): kwargs['local_files_only'] = True
    return orig_model_load.__func__(cls, LOCAL_SAFE_PATH if is_target_model(path) else path, *args, **kwargs)

def mocked_tok_load(cls, path, *args, **kwargs):
    if is_target_model(path): kwargs['local_files_only'] = True
    return orig_tok_load.__func__(cls, LOCAL_SAFE_PATH if is_target_model(path) else path, *args, **kwargs)

# æ¿€æ´»æ‹¦æˆª
AutoConfig.from_pretrained = classmethod(mocked_config_load)
AutoModelForSequenceClassification.from_pretrained = classmethod(mocked_model_load)
AutoTokenizer.from_pretrained = classmethod(mocked_tok_load)

#---------------- vllmæ‹¦æˆª ----------------
try:
    import vllm
    orig_vllm_init = vllm.LLM.__init__

    def mocked_vllm_init(self, model, *args, **kwargs):
        if model == 'Bespoke-MiniCheck-7B' or 'Bespoke' in str(model):
            print(f"ğŸ›¡ï¸ [vLLM æ‹¦æˆªå™¨] æ­£åœ¨å°†æ¨¡å‹é‡å®šå‘åˆ°æœ¬åœ° -> {LOCAL_SAFE_PATH}")
            model = LOCAL_SAFE_PATH
            kwargs['trust_remote_code'] = True
            kwargs['dtype'] = "bfloat16"
            kwargs['gpu_memory_utilization'] = 0.6
        
        return orig_vllm_init(self, model, *args, **kwargs)

    vllm.LLM.__init__ = mocked_vllm_init
    print("âœ… vLLM æ‹¦æˆªå™¨å·²æ¿€æ´» (vLLM Interceptor Activated)")

except ImportError:
    print("âš ï¸ æœªæ£€æµ‹åˆ° vLLM åº“,è·³è¿‡ vLLM æ‹¦æˆª (å¯èƒ½æ­£åœ¨ä½¿ç”¨ Torch æ¨¡å¼)")

# 4. NLTK ç¦»çº¿è¡¥ä¸
import nltk.tokenize
def apply_nltk_patch():
    pickle_path = os.path.expanduser("~/nltk_data/tokenizers/punkt/english.pickle")
    if not os.path.exists(pickle_path):
        pickle_path = "/root/nltk_data/tokenizers/punkt/english.pickle"
    
    if os.path.exists(pickle_path):
        try:
            with open(pickle_path, 'rb') as f:
                tokenizer = pickle.load(f)
            nltk.tokenize.sent_tokenize = lambda t, language='english': tokenizer.tokenize(t)
        except Exception as e:
            logging.warning(f"NLTK patch failed: {e}")
apply_nltk_patch()

try:
    from minicheck.minicheck import MiniCheck
except ImportError:
    MiniCheck = None

# ================= 1. é…ç½® (Configuration) =================
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger(__name__)


llm = None
_MINICHECK_SCORER = None
_BLOOM_FILTER = None
_GIBBERISH_DETECTOR = None


# ================= Dynamic Few-Shot Selector (é€‚é…ä½ çš„ JSON) =================

_FEW_SHOT_SELECTOR = None

# ä¿®æ”¹ DynamicFewShotSelector ç±»å¦‚ä¸‹ï¼š

class DynamicFewShotSelector:
    def __init__(self, examples_path, model_path_or_name):
        if not HAS_EMBEDDING:
            raise ImportError("è¯·å…ˆå®‰è£…ä¾èµ–: pip install sentence-transformers onnxruntime")
            
        # ============ [ä¿®æ”¹ç‚¹] åŠ¨æ€åŠ è½½é€»è¾‘ ============
        if AppConfig.EMBEDDING_USE_ONNX:
            # æ¨¡å¼ A: ä½¿ç”¨ ONNX (æœ¬åœ°è·¯å¾„)
            self.model = OnnxBgeEmbedder(model_path_or_name)
        else:
            # æ¨¡å¼ B: ä½¿ç”¨ PyTorch (æœ¬åœ°è·¯å¾„ æˆ– HuggingFace ID)
            logger.info(f"æ­£åœ¨åŠ è½½ PyTorch Embedding æ¨¡å‹: {model_path_or_name} ...")
            self.model = SentenceTransformer(model_path_or_name, device='cpu')
        # ============================================
        
        with open(examples_path, 'r', encoding='utf-8') as f:
            self.examples = json.load(f)
            
        logger.info(f"æ­£åœ¨ä¸º {len(self.examples)} æ¡åˆ¤ä¾‹æ„å»ºç´¢å¼•...")
        
        # 1. æ„å»ºè¯­æ–™åº“
        self.corpus = []
        for ex in self.examples:
            # é€‚é…ä½ çš„ json å­—æ®µ
            ref = ex.get("reference", "")
            q = ex.get("question", "")
            ans = ex.get("answer", "")
            # æ‹¼æ¥ Ref+Q+A ä»¥è·å¾—æœ€ä½³æ£€ç´¢æ•ˆæœ
            text_to_embed = f"Ref: {ref}\nQuestion: {q}\nAnswer: {ans}"
            self.corpus.append(text_to_embed)
            
        # 2. è®¡ç®—å‘é‡ (encode æ¥å£å·²ç»Ÿä¸€)
        # æ³¨æ„ï¼šSentenceTransformer é»˜è®¤è¿”å› Tensor æˆ– numpyï¼ŒONNX è¿”å› numpy
        self.embeddings = self.model.encode(self.corpus)
        
        # ç¡®ä¿è½¬ä¸º numpy ä¸”å½’ä¸€åŒ– (é˜²æ­¢ SentenceTransformer æœªå½’ä¸€åŒ–)
        if hasattr(self.embeddings, "detach"): # å¦‚æœæ˜¯ Tensor
             self.embeddings = self.embeddings.detach().cpu().numpy()
             
        # å†æ¬¡å¼ºåˆ¶å½’ä¸€åŒ– (åŒé‡ä¿é™©)
        norm = np.linalg.norm(self.embeddings, axis=1, keepdims=True)
        self.embeddings = self.embeddings / (norm + 1e-9)
        
        logger.info("åˆ¤ä¾‹åº“ç´¢å¼•æ„å»ºå®Œæˆï¼")

    def retrieve(self, current_ref, current_q, current_ans, k=2):
        """æ£€ç´¢æœ€ç›¸ä¼¼çš„ k ä¸ªä¾‹å­"""
        query = f"Ref: {current_ref}\nQuestion: {current_q}\nAnswer: {current_ans}"
        
        # å‘é‡åŒ–æŸ¥è¯¢
        query_vec = self.model.encode([query])
        
        # ç¡®ä¿æ ¼å¼ç»Ÿä¸€
        if hasattr(query_vec, "detach"):
            query_vec = query_vec.detach().cpu().numpy()
            
        # å½’ä¸€åŒ–
        query_vec = query_vec / (np.linalg.norm(query_vec, axis=1, keepdims=True) + 1e-9)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        scores = np.dot(query_vec, self.embeddings.T)[0]
        top_indices = np.argsort(scores)[::-1][:k]
        
        return [self.examples[i] for i in top_indices]

# ä¿®æ”¹åŠ è½½å‡½æ•°ï¼Œä¼ å…¥æ–°çš„ PATH é…ç½®
def _ensure_selector_loaded():
    global _FEW_SHOT_SELECTOR
    if not AppConfig.ENABLE_DYNAMIC_FEW_SHOT:
        return False
    
    if _FEW_SHOT_SELECTOR is not None:
        return True
        
    try:
        # è¿™é‡Œä¼ å…¥ AppConfig.EMBEDDING_MODEL_PATH (æœ¬åœ°è·¯å¾„)
        _FEW_SHOT_SELECTOR = DynamicFewShotSelector(
            AppConfig.FEW_SHOT_EXAMPLES_PATH,
            AppConfig.EMBEDDING_MODEL_PATH 
        )
        return True
    except Exception as e:
        logger.error(f"Few-Shot Selector åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
        return False

# ================= [æ–°å¢] ONNX Embedding åŒ…è£…å™¨ =================

class OnnxBgeEmbedder:
    """
    è½»é‡çº§ ONNX æ¨ç†åŒ…è£…å™¨ï¼Œæ¥å£å…¼å®¹ SentenceTransformer
    ä¸“é—¨ç”¨äºåŠ è½½æœ¬åœ° bge-m3.onnx
    """
    def __init__(self, model_dir: str, model_filename: str = "model.onnx"):
        if not ort or not AutoTokenizer:
            raise ImportError("è¯·å®‰è£… onnxruntime å’Œ transformers: pip install onnxruntime transformers")
        
        logger.info(f"æ­£åœ¨åŠ è½½æœ¬åœ° ONNX æ¨¡å‹: {model_dir} ...")
        
        # 1. åŠ è½½ Tokenizer (è´Ÿè´£æŠŠæ–‡æœ¬è½¬æˆ ID)
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        
        # 2. åŠ è½½ ONNX Session (è´Ÿè´£æ¨ç†)
        model_path = os.path.join(model_dir, model_filename)
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ONNX æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")
            
        # ä½¿ç”¨ CPU æ¨ç† (Embedding é€šå¸¸å¾ˆå¿«ï¼ŒCPU è¶³å¤Ÿ)
        providers = ['CPUExecutionProvider']
        # å¦‚æœä½ æœ‰ GPU ä¸”è£…äº† onnxruntime-gpuï¼Œå¯ä»¥æŠŠ 'CUDAExecutionProvider' æ”¾å‰é¢
        
        self.session = ort.InferenceSession(model_path, providers=providers)
        
    def encode(self, sentences: list, batch_size: int = 4, **kwargs):
        """
        å…¼å®¹ SentenceTransformer çš„ encode æ¥å£
        """
        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨
        if isinstance(sentences, str):
            sentences = [sentences]
            
        all_embeddings = []
        
        # æ‰¹é‡å¤„ç†
        for i in range(0, len(sentences), batch_size):
            batch_texts = sentences[i : i + batch_size]
            
            # 1. Tokenization (BGE-M3 æ”¯æŒé•¿æ–‡æœ¬ï¼Œè¿™é‡Œè®¾ 8192 æˆ– 1024 å‡å¯)
            inputs = self.tokenizer(
                batch_texts, 
                padding=True, 
                truncation=True, 
                max_length=1024, 
                return_tensors="np" # ç›´æ¥è¿”å› numpy æ•°ç»„ç»™ ONNX ç”¨
            )
            
            # 2. æ„é€  ONNX è¾“å…¥
            # æ³¨æ„ï¼šBGE-M3 (XLM-RoBERTa) é€šå¸¸åªéœ€è¦ input_ids å’Œ attention_mask
            ort_inputs = {
                "input_ids": inputs["input_ids"].astype(np.int64),
                "attention_mask": inputs["attention_mask"].astype(np.int64)
            }
            
            # 3. ONNX æ¨ç†
            # output[0] é€šå¸¸æ˜¯ last_hidden_state (Batch, SeqLen, Dim)
            outputs = self.session.run(None, ort_inputs)
            last_hidden_state = outputs[0]
            
            # 4. Pooling & Normalization
            # BGE-M3 çš„ Dense Embedding ä½¿ç”¨ CLS token (ç´¢å¼• 0)
            embeddings = last_hidden_state[:, 0, :]
            
            # å½’ä¸€åŒ– (L2 Norm) - è¿™ä¸€æ­¥å¯¹äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦è‡³å…³é‡è¦
            norm = np.linalg.norm(embeddings, axis=1, keepdims=True)
            embeddings = embeddings / (norm + 1e-9)
            
            all_embeddings.append(embeddings)
            
        if not all_embeddings:
            return np.array([])
            
        # åˆå¹¶æ‰€æœ‰ batch
        return np.vstack(all_embeddings)

# ================= ç¨³å®šå“ˆå¸Œå‡½æ•° (å¿…é¡»ä¸ build_filter.py ä¸€è‡´) =================
def stable_hash(val):
    """
    å¿…é¡»ä¸ build_filter.py ä¸­çš„å“ˆå¸Œå‡½æ•°å®Œå…¨ä¸€è‡´
    """
    if isinstance(val, str):
        val = val.encode('utf-8')
    h_int = int(hashlib.md5(val).hexdigest(), 16)
    if h_int >= 1 << 127:
        h_int -= 1 << 128
    return h_int


def _ensure_bloom_loaded():
    """å»¶è¿ŸåŠ è½½ Bloom Filter (å¸¦å“ˆå¸Œå‡½æ•°ä¿®å¤)"""
    global _BLOOM_FILTER
    if not AppConfig.ENABLE_BLOOM_FILTER or Bloom is None:
        return False
    if _BLOOM_FILTER is not None:
        return True
    
    bloom_path = Path(AppConfig.BLOOM_FILTER_PATH)
    if not bloom_path.exists():
        logger.warning(f"Bloom filter file not found: {bloom_path}")
        return False
    
    try:
        # ã€æ ¸å¿ƒä¿®å¤ã€‘: ä¼ å…¥ hash_func=stable_hash
        if 'rbloom' in str(Bloom):
            _BLOOM_FILTER = Bloom.load(str(bloom_path), hash_func=stable_hash)
        else:
            # pybloom_live é€šå¸¸æŠŠ hash å­˜è¿›å»äº†,ç›´æ¥åŠ è½½å³å¯
            with open(bloom_path, 'rb') as f:
                _BLOOM_FILTER = pickle.load(f)
                
        logger.info(f"âœ… Bloom Filter loaded from {bloom_path}")
        return True
    except Exception as e:
        logger.error(f"Failed to load Bloom Filter: {e}")
        return False


def _ensure_gibberish_loaded():
    """å»¶è¿ŸåŠ è½½ Gibberish Detector"""
    global _GIBBERISH_DETECTOR
    if not AppConfig.ENABLE_GIBBERISH_CHECK or not HAS_GIBBERISH:
        return False
    if _GIBBERISH_DETECTOR is not None:
        return True
    
    try:
        _GIBBERISH_DETECTOR = gibberish_detector.Detector()
        logger.info("âœ… Gibberish Detector loaded")
        return True
    except Exception as e:
        logger.error(f"Failed to load Gibberish Detector: {e}")
        return False


def _ensure_minicheck_loaded():
    global _MINICHECK_SCORER
    if not AppConfig.MINICHECK_ENABLED or MiniCheck is None:
        return False
    if _MINICHECK_SCORER is not None:
        return True
    try:
        _MINICHECK_SCORER = MiniCheck(
            model_name=AppConfig.MINICHECK_MODEL_NAME,
            enable_prefix_caching=AppConfig.MINICHECK_ENABLE_PREFIX_CACHING
        )
        logger.info(f"MiniCheck loaded successfully: {AppConfig.MINICHECK_MODEL_NAME}")
        return True
    except Exception as e:
        logger.error(f"MiniCheck Init Failed: {e}", exc_info=True)
        return False


# ================= 2. æ™ºèƒ½é¢„å¤„ç† (Smart Preprocessing) =================

def smart_extract(text: str) -> Tuple[str, Optional[str]]:
    """
    [å‡çº§ç‰ˆ] ä¸‰çº§å¼•ç”¨æå–ç­–ç•¥:
    Level 1: æ˜¾å¼æ ‡ç­¾ ([Reference]...)
    Level 2: ä»£ç å—åŠ«æŒ (```...```)
    Level 3: è‡ªç„¶è¯­è¨€å‰ç¼€ (Based on...)
    Level 4: ç»“æ„åŒ–å…œåº• (é•¿æ–‡åˆ‡åˆ†)
    """
    if not text:
        return "", None
    clean_text = text.strip()

    # --- Level 1: æ˜¾å¼æ ‡ç­¾åŒ¹é… (ä¼˜å…ˆçº§æœ€é«˜) ---
    patterns_brackets = [
        (r"\[Reference\]", r"\[Question\]"),
        (r"Reference:", r"Question:"),
        (r"Context:", r"Query:"),
        (r"ã€å‚è€ƒèµ„æ–™ã€‘", r"ã€é—®é¢˜ã€‘"),
        (r"èµ„æ–™ï¼š", r"é—®é¢˜ï¼š"),
    ]
    for ref_tag, q_tag in patterns_brackets:
        ref_match = re.search(ref_tag, clean_text, re.IGNORECASE)
        q_match = re.search(q_tag, clean_text, re.IGNORECASE)
        if ref_match and q_match:
            # æ”¯æŒ Ref åœ¨å‰ æˆ– Ref åœ¨å
            if ref_match.start() < q_match.start():
                q = clean_text[q_match.end():].strip()
                ref = clean_text[ref_match.end(): q_match.start()].strip()
                return q, ref
            else:
                q = clean_text[q_match.end(): ref_match.start()].strip()
                ref = clean_text[ref_match.end():].strip()
                return q, ref

    # --- Level 2: ä»£ç å—åŠ«æŒ ---
    # åŒ¹é…ä¸‰ç§å¸¸è§æ ‡è®°ï¼š```, ''', ~~~ï¼Œå¹¶åˆ©ç”¨ \1 ç¡®ä¿é¦–å°¾æ ‡è®°ä¸€è‡´
    # findall è¿”å›çš„æ˜¯ [(æ ‡è®°, å†…å®¹), (æ ‡è®°, å†…å®¹)...] çš„åˆ—è¡¨
    code_matches = re.findall(r"(```|'''|~~~)([\s\S]*?)\1", clean_text)
    
    if code_matches:
        # æ‰¾åˆ°å†…å®¹æœ€é•¿çš„é‚£ä¸€ç»„åŒ¹é… (m[1] æ˜¯å†…å®¹)
        longest_match = max(code_matches, key=lambda m: len(m[1]))
        delimiter = longest_match[0]   # è·å–è¯¥æ®µä»£ç ç”¨çš„æ ‡è®° (æ¯”å¦‚ ''')
        code_content = longest_match[1] # è·å–ä¸­é—´çš„ä»£ç å†…å®¹
        
        # ä¾ç„¶ä¿æŒ 50 å­—ç¬¦çš„é˜ˆå€¼åˆ¤æ–­
        if len(code_content) > 50:
            # ç»„è£…å‡ºå®Œæ•´çš„å—å­—ç¬¦ä¸²ï¼ˆæ ‡è®° + å†…å®¹ + æ ‡è®°ï¼‰ï¼Œä»¥ä¾¿ä»åŸæ–‡ä¸­ç²¾ç¡®ç§»é™¤
            full_block_str = delimiter + code_content + delimiter
            
            # ä»åŸæ–‡ä¸­ç§»é™¤è¿™æ®µä»£ç 
            q = clean_text.replace(full_block_str, "").strip()
            
            return q, code_content.strip()

    # --- Level 3: è‡ªç„¶è¯­è¨€è§¦å‘è¯ (æ–°å¢) ---
    nl_patterns = [
        r"^(?:Based on|According to|Given|Refer to|Read) (?:the following|the text|the article|the passage)[:,\s]",
        r"^Here is (?:a|the) (?:text|article|passage)[:,\s]",
        r"^(?:åŸºäº|æ ¹æ®|å‚è€ƒ|é˜…è¯»|ä¾æ®)(?:ä»¥ä¸‹|ä¸‹æ–‡|ä¸Šè¿°)?(?:èµ„æ–™|å†…å®¹|æ–‡ç« |æ®µè½|æ–‡æœ¬)?[:ï¼š,\s]",
    ]
    for pat in nl_patterns:
        if re.match(pat, clean_text, re.IGNORECASE):
            # å°è¯•å¯»æ‰¾æœ€åçš„é—®é¢˜åˆ†éš”ç¬¦
            splitters = [r"\nQuestion:", r"\nQuery:", r"\né—®é¢˜ï¼š", r"\nä»»åŠ¡ï¼š"]
            for sp in splitters:
                parts = re.split(sp, clean_text, flags=re.IGNORECASE)
                if len(parts) >= 2:
                    return parts[-1].strip(), parts[0].strip()

            # å°¾éƒ¨åˆ‡åˆ†æ³•: å‡è®¾æœ€åä¸€æ®µæ˜¯é—®é¢˜
            segments = re.split(r'\n\s*\n', clean_text)
            if len(segments) >= 2:
                q = segments[-1].strip()
                ref = "\n".join(segments[:-1]).strip()
                if len(ref) > len(q) and len(ref) > 20:
                    return q, ref

    # --- Level 4: ç»“æ„åŒ–å…œåº• ---
    # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä½†æ–‡æœ¬å¾ˆé•¿ä¸”ç»“å°¾åƒé—®é¢˜
    if len(clean_text) > 300:
        lines = clean_text.split('\n')
        last_line = lines[-1].strip()
        if len(last_line) < 150 and (last_line.endswith('?') or "æ€»ç»“" in last_line):
             return last_line, "\n".join(lines[:-1]).strip()

    return clean_text, None


def clean_and_check_refusal(
    question: str, answer: str, strict_mode: bool
) -> Tuple[Optional[dict], str]:
    """Refusal å¤„ç†é€»è¾‘"""
    q_lower = (question or "").lower()

    code_triggers = [
        "write code", "function", "script", "javascript", "python", "java", "c++", "algorithm",
        "program", "implement", "develop", "html", "css", "sql", "query", "render", "draw",
        "canvas", "excel formula", "regex", "regular expression",
        "å†™ä»£ç ", "å‡½æ•°", "è„šæœ¬", "ç®—æ³•", "ç¼–ç¨‹", "ç¨‹åº", "å®ç°", "å¼€å‘", "æ¸²æŸ“", "ç»˜åˆ¶", "æ­£åˆ™",
    ]
    code_triggers = [t.lower() for t in code_triggers]
    if any(t in q_lower for t in code_triggers):
        return None, answer

    scan = (answer or "")[: AppConfig.REFUSAL_SCAN_CHARS]

    patterns = [
        r"^\s*((?:as an ai|as a language model|i cannot|i can't|unable to|i don't have|i do not have|i don't know|i have no|my knowledge)[\s\S]{0,320}?)(?:\s*(?:however|but|here is|below is|i can provide)\b[\s\S]{0,120}.*)$",
        r"^\s*((?:æˆ‘æ˜¯ai|äººå·¥æ™ºèƒ½|è¯­è¨€æ¨¡å‹|æ— æ³•|ä¸èƒ½|ä¸çŸ¥é“|æˆ‘æ²¡æœ‰|æˆ‘æ— æ³•|æˆ‘ä¸èƒ½)[\s\S]{0,320}?)(?:\s*(?:ä½†æ˜¯|ä¸è¿‡|ç„¶è€Œ|ä»¥ä¸‹æ˜¯|æˆ‘å¯ä»¥æä¾›)\b[\s\S]{0,120}.*)$",
    ]

    CONTEXT_KEYWORDS = [
        "text", "image", "context", "input", "provide", "access", "browse", "internet",
        "real-time", "realtime", "file", "see", "passage", "reference", "given", "mentioned",
        "data", "article", "local date", "system time", "camera",
        "æ–‡æœ¬", "å›¾ç‰‡", "ä¸Šä¸‹æ–‡", "è¾“å…¥", "æä¾›", "è®¿é—®", "æµè§ˆ", "è”ç½‘", "å®æ—¶", "æ–‡ä»¶", "çœ‹",
        "æ–‡ç« ", "å‚è€ƒ", "ç»™å®š", "æåŠ", "èµ„æ–™", "æœ¬åœ°æ—¶é—´", "ç³»ç»Ÿæ—¶é—´",
    ]

    OPINION_KEYWORDS = [
        "opinion", "opinions", "belief", "beliefs", "feel", "personal view", "standpoint",
        "sentience", "consciousness",
        "è§‚ç‚¹", "çœ‹æ³•", "ç«‹åœº", "ä¸ªäººè®¤ä¸º", "ä¿¡ä»°", "æ„Ÿå—", "æ„è¯†",
    ]

    clean_answer = answer or ""

    for pat in patterns:
        m = re.match(pat, scan, re.IGNORECASE | re.DOTALL)
        if not m:
            continue

        waste_words = m.group(1) or ""
        refusal_part = waste_words.lower()

        is_context_refusal = any(kw in refusal_part for kw in CONTEXT_KEYWORDS)
        is_opinion_refusal = any(kw in refusal_part for kw in OPINION_KEYWORDS)

        if is_opinion_refusal:
            cleaned = clean_answer.replace(waste_words, "", 1).strip()
            cleaned = re.sub(r"^(however|but|ä½†æ˜¯|ä¸è¿‡|ç„¶è€Œ)[,,, \s]*", "", cleaned, flags=re.IGNORECASE).strip()
            return None, cleaned

        if is_context_refusal:
            if strict_mode:
                return (
                    {
                        "status": "FAIL",
                        "risk_level": "MEDIUM",
                        "reason": f"ä¸¥æ ¼æ¨¡å¼:æ£€æµ‹åˆ°ä¸Šä¸‹æ–‡/èƒ½åŠ›å—é™å¼æ‹’ç­”è½¬æŠ˜,ç–‘ä¼¼é€»è¾‘å†²çªæˆ–æ¥æºä¸å¯è¿½æº¯: {refusal_part[:80]}...",
                    },
                    answer,
                )
            cleaned = clean_answer.replace(waste_words, "", 1).strip()
            cleaned = re.sub(r"^(however|but|ä½†æ˜¯|ä¸è¿‡|ç„¶è€Œ)[,,, \s]*", "", cleaned, flags=re.IGNORECASE).strip()
            return None, cleaned

        if strict_mode:
            return (
                {
                    "status": "FAIL",
                    "risk_level": "LOW",
                    "reason": f"ä¸¥æ ¼æ¨¡å¼:æ£€æµ‹åˆ°ä¸æ˜åŸå› æ‹’ç­”è½¬æŠ˜,ç–‘ä¼¼é€»è¾‘å†²çª: {refusal_part[:80]}...",
                },
                answer,
            )
        cleaned = clean_answer.replace(waste_words, "", 1).strip()
        cleaned = re.sub(r"^(however|but|ä½†æ˜¯|ä¸è¿‡|ç„¶è€Œ)[,,, \s]*", "", cleaned, flags=re.IGNORECASE).strip()
        return None, cleaned

    return None, clean_answer


# ================= 3. åŸå­åŒ– Prompts =================

class RobustJsonParser(BaseOutputParser):
    def parse(self, text: str) -> dict:
        raw = text or ""
        clean = re.sub(r"<think>.*?</think>", "", raw, flags=re.DOTALL)
        clean = clean.replace("```json", "").replace("```", "").strip()

        try:
            return json.loads(clean)
        except Exception:
            pass

        try:
            matches = re.findall(r"(\{[\s\S]*\})", clean, re.DOTALL)
            if matches:
                return json.loads(matches[-1])
        except Exception:
            pass

        return {
            "status": "FAIL",
            "reason": "Output parsing error (non-JSON response).",
            "raw": raw[:1500],
        }


def standardize_result(raw_data: dict) -> dict:
    data = {k.lower(): v for k, v in (raw_data or {}).items()}
    status = str(data.get("status", "FAIL")).upper()
    status = "PASS" if "PASS" in status else "FAIL"
    reason = str(data.get("step_3_verdict", data.get("reason", data.get("analysis", "No reason"))))
    risk = str(data.get("risk_level", "LOW")).upper()
    if risk not in {"LOW", "MEDIUM", "HIGH"}:
        risk = "LOW"
    return {"status": status, "risk_level": risk, "reason": reason, "trace": raw_data}


INTENT_PROMPT = """ä»»åŠ¡ï¼šåˆ†æç”¨æˆ·æŒ‡ä»¤çš„æ„å›¾ç±»å‹ã€‚
ã€ç”¨æˆ·æŒ‡ä»¤ã€‘: {question}

è¯·ä»ä»¥ä¸‹ 4 ä¸ªç±»åˆ«ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€ä¸ªï¼Œè¾“å‡º JSON: {{"type": "CATEGORY"}}

1. CREATIVE (å¼€æ”¾åˆ›ä½œ):
   - å…³é”®è¯: åˆ›ä½œã€å†™æ•…äº‹ã€å†™è¯—ã€å‡è®¾ã€æ‰®æ¼”ã€ç»­å†™ã€é‚®ä»¶ã€æ–‡æ¡ˆã€‚
   - ç¤ºä¾‹: "å†™ä¸€ä¸ªç§‘å¹»æ•…äº‹", "å¸®æˆ‘å†™å°è¯·å‡æ¡", "å‡è®¾ä½ æ˜¯é©¬æ–¯å…‹"ã€‚
2. CONSTRAINED (åŸºäºä¸Šæ–‡):
   - å¼ºä¾èµ–ç»™å®šçš„å‚è€ƒèµ„æ–™ (ä½†æ³¨æ„ï¼šå¦‚æœç”¨æˆ·æ²¡æœ‰æä¾›èµ„æ–™ï¼Œç»å¯¹ä¸è¦é€‰è¿™ä¸ªï¼)ã€‚
3. CODE (ä»£ç ç¼–ç¨‹):
   - ç¼–å†™ã€è§£é‡Šæˆ–è°ƒè¯•ä»£ç /SQL/æ­£åˆ™ã€‚
4. QA (äº‹å®é—®ç­”):
   - è¯¢é—®å®¢è§‚äº‹å®ã€çŸ¥è¯†ã€æ¦‚å¿µå®šä¹‰ã€‚
   - ç¤ºä¾‹: "é²è¿…æ˜¯è°ï¼Ÿ", "è‹¹æœçš„è‚¡ä»·æ˜¯å¤šå°‘ï¼Ÿ", "æ¨èå‡ éƒ¨ç”µå½±"ã€‚
"""

CONSTRAINED_AUDIT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸¥è‹›çš„"äº‹å®æ ¸æŸ¥æ³•å®˜"ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åŸºäºã€å‚è€ƒèµ„æ–™ã€‘,å®¡åˆ¤ã€æ¨¡å‹å›ç­”ã€‘æ˜¯å¦å­˜åœ¨**ä»»ä½•å½¢å¼çš„å¹»è§‰æˆ–ä¸å¿ å®**ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘
{reference}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æ¨¡å‹å›ç­”ã€‘
{answer}

è¯·åŸºäºä»¥ä¸‹"ä¸‰å¤§é“å¾‹"è¿›è¡Œå®¡æŸ¥ã€‚åªè¦è¿åä»»ä½•ä¸€æ¡,å³åˆ¤ä¸º FAIL:

1. **é“å¾‹ä¸€:ä¸¥ç¦æ— ä¸­ç”Ÿæœ‰ (The "Not-Mentioned" Rule)**
   - **å®¡æŸ¥ç›®æ ‡**:æ£€æŸ¥å›ç­”ä¸­æ˜¯å¦å‡ºç°äº†åŸæ–‡æœªæåŠçš„**å…·ä½“å®ä½“**(äººåã€åœ°åã€æœºæ„)ã€**æ•°å€¼**(æ—¥æœŸã€ç™¾åˆ†æ¯”ã€é‡‘é¢)æˆ–**äº‹ä»¶**ã€‚
   - **å¹»è§‰ç±»å‹**:äº‹å®æ–°å¢ã€ç»†èŠ‚ç¼–é€ ã€å¼•ç”¨é”™é…ã€‚

2. **é“å¾‹äºŒ:ä¸¥ç¦é€»è¾‘æ‰­æ›² (The "Logic-Twist" Rule)**
   - **å®¡æŸ¥ç›®æ ‡**:æ£€æŸ¥å›ç­”æ˜¯å¦é¢ å€’äº†åŸæ–‡çš„**å› æœå…³ç³»**ã€**ä¸»è¢«åŠ¨å…³ç³»**æˆ–**è‚¯å®š/å¦å®š**ã€‚
   - **å¹»è§‰ç±»å‹**:äº‹å®æ­ªæ›²ã€å› æœè¡¥å…¨ã€‚

3. **é“å¾‹ä¸‰:ä¸¥ç¦ç¨‹åº¦/èŒƒå›´æ¼‚ç§» (The "Scope-Drift" Rule)**
   - **å®¡æŸ¥ç›®æ ‡**:æ£€æŸ¥å›ç­”æ˜¯å¦å°†åŸæ–‡çš„"å¯èƒ½/éƒ¨åˆ†"å¤¸å¤§ä¸º"ä¸€å®š/æ‰€æœ‰",æˆ–å¿½ç•¥äº†åŸæ–‡çš„å‰ææ¡ä»¶ã€‚
   - **å¹»è§‰ç±»å‹**:è¿‡åº¦æ¦‚æ‹¬ã€è¯­ä¹‰æ¼‚ç§»ã€é€‰æ‹©æ€§å¿½ç•¥ã€‚

è¯·è¾“å‡º JSON:
{{
    "analysis": "1. å®ä½“æ ¸æŸ¥: [æœ‰æ— æ–°å¢]... 2. é€»è¾‘æ ¸æŸ¥: [æœ‰æ— çŸ›ç›¾]... 3. ç¨‹åº¦æ ¸æŸ¥: [æœ‰æ— å¤¸å¤§/èŒƒå›´é”™è¯¯]...",
    "status": "PASS" | "FAIL",
    "reason": "è¯·æ˜ç¡®æŒ‡å‡ºè¿åäº†å“ªä¸€æ¡é“å¾‹"
}}
"""

AUGMENTED_CONSTRAINED_PROMPT = """ä½ æ˜¯ä¸€åèµ„æ·±çš„"äº‹å®ä»²è£æ³•å®˜"ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åŸºäºã€å‚è€ƒèµ„æ–™ã€‘,å¯¹ã€æ¨¡å‹å›ç­”ã€‘è¿›è¡Œç»ˆå®¡ã€‚

æˆ‘ä»¬ä½¿ç”¨åˆçº§è‡ªåŠ¨åŒ–å·¥å…·å¯¹å›ç­”è¿›è¡Œäº†é¢„æ‰«æ,å¹¶æ ‡è®°äº†ä¸€äº›ç–‘ä¼¼é£é™©ç‚¹ã€‚è¯·å‚è€ƒä»¥ä¸‹**ä¸‰å¤§é“å¾‹**ä¸**åˆ¤ä¾‹**è¿›è¡Œæœ€ç»ˆè£å†³ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘
{reference}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æ¨¡å‹å›ç­”ã€‘
{answer}

ã€ğŸ” é¢„æ‰«æé«˜äº®æç¤ºã€‘
(ä»¥ä¸‹å¥å­è¢«å·¥å…·æ ‡è®°ä¸ºç¼ºä¹æ”¯æ’‘,è¯·é‡ç‚¹å¤æ ¸,ä½†ä¸è¦ç›²ä»)
{nli_evidence}

### å®¡åˆ¤æ³•åˆ™ (The Iron Rules)
**åªè¦è¿åä»»ä½•ä¸€æ¡ï¼Œå³åˆ¤ä¸º FAIL**ï¼š

1. **é“å¾‹ä¸€: ä¸¥ç¦æ— ä¸­ç”Ÿæœ‰ (The "Not-Mentioned" Rule)**
   - **çº¢çº¿**: ä¸¥ç¦å¼•å…¥åŸæ–‡æœªæåŠçš„**å…·ä½“å®ä½“**(äººå/å¤´è¡”)ã€**æ•°å€¼**ã€**äº‹ä»¶**æˆ–**åŠ¨ä½œ**ã€‚
   - **ä¾‹å¤–**: å…è®¸åŒä¹‰æ”¹å†™ (released -> launched) å’Œ å¸¸è¯†æ€§ä»£è¯æŒ‡ä»£ã€‚
   - **è±å…**: å¦‚æœæ¨¡å‹å›ç­”æ˜¯å¯¹åŸæ–‡çš„**æ€»ç»“(Summarization)**æˆ–**è·¨æ®µè½æ•´åˆ(Synthesis)**ï¼Œåªè¦æ ¸å¿ƒäº‹å®ä¸å†²çªï¼Œ**å…è®¸**çœç•¥éå…³é”®ç»†èŠ‚ï¼Œ**å…è®¸**åŒä¹‰è¡¨è¿°ã€‚ä¸è¦ä»…ä»…å› ä¸ºâ€œåŸæ–‡æ²¡åŸè¯â€å°±åˆ¤é”™ã€‚

2. **é“å¾‹äºŒ: ä¸¥ç¦é€»è¾‘æ‰­æ›² (The "Logic-Twist" Rule)**
   - **çº¢çº¿**: ä¸¥ç¦é¢ å€’å› æœã€ä¸»è¢«åŠ¨æˆ–è‚¯å®š/å¦å®šå…³ç³»ï¼Œä¸¥ç¦æ—¶ç©ºé”™ä¹±å’Œæ•°å­¦è®¡ç®—é”™è¯¯ã€‚

3. **é“å¾‹ä¸‰: ä¸¥ç¦ç¨‹åº¦æ¼‚ç§» (The "Scope-Drift" Rule)**
   - **çº¢çº¿**: ä¸¥ç¦å°†"å¯èƒ½"å¤¸å¤§ä¸º"ä¸€å®š"ï¼Œæˆ–å¿½ç•¥å‰ææ¡ä»¶ã€‚

### åˆ¤ä¾‹ç¤ºèŒƒ (Few-Shot Demonstrations)
å­¦ä¹ ä»¥ä¸‹ä¸¤ä¸ªæ¡ˆä¾‹ï¼š

{few_shot_examples}

---

ã€ä»²è£å®¡æŸ¥æ­¥éª¤ã€‘
1. **å…¨æ–‡é€šè¯»ä¸æ¶ˆæ­§**: é¦–å…ˆé€šè¯»å…¨æ–‡,ç¡®è®¤ä»£è¯(ä»–/å®ƒ)æŒ‡ä»£çš„å¯¹è±¡,å¹¶ç†è§£æ•´æ®µå›ç­”çš„é€»è¾‘é“¾æ¡ã€‚
2. **ç–‘ç‚¹è¯­å¢ƒå¤æ ¸**: 
   - å°†"é«˜äº®æç¤º"ä¸­çš„å¥å­æ”¾å›ã€å‚è€ƒèµ„æ–™ã€‘çš„**åŸå§‹è¯­å¢ƒ**ä¸­æ¯”å¯¹ã€‚
   - **å…³é”®åˆ¤æ–­**: è¿™æ˜¯"æ— ä¸­ç”Ÿæœ‰"çš„é”™è¯¯?è¿˜æ˜¯"åŒä¹‰è½¬æ¢"çš„åˆç†æ”¹å†™?
3. **æœ€ç»ˆåˆ¤å†³**:
   - æ ¹æ®è§„åˆ™ç»™å‡ºä½ çš„åˆ¤æ–­ (PASS æˆ– FAIL)ã€‚

è¾“å‡º JSON:
{{
  "step_1_context": "ç®€è¿°å…¨æ–‡æ ¸å¿ƒé€»è¾‘...",
  "step_2_evidence_audit": "é’ˆå¯¹é«˜äº®å¥çš„å¤æ ¸åˆ†æ(æ˜¯è¯¯æŠ¥è¿˜æ˜¯å®é”¤)...",
  "step_3_verdict": "æœ€ç»ˆåˆ¤å®š (PASS æˆ– FAIL)",
  "status": "PASS" | "FAIL",
  "reason": "..."
}}
"""

# ================= æ ¸å¿ƒä¿®æ”¹: V5 èåˆç‰ˆå®¡è®¡ Prompt =================
# ç»“åˆäº†"çŸ¥è¯†æ³¨å…¥"çš„å‡†ç¡®æ€§ + "å®ä½“å®¡è®¡"çš„å®¡åˆ¤é€»è¾‘
KNOWLEDGE_RETRIEVAL_PROMPT = """ä½ æ˜¯ä¸€ä¸ªç™¾ç§‘å…¨ä¹¦ã€‚è¯·æ ¹æ®ä½ çš„å†…éƒ¨çŸ¥è¯†ï¼Œä¸ºå›ç­”ç”¨æˆ·çš„é—®é¢˜æä¾›å¿…è¦çš„çŸ¥è¯†æ”¯æ’‘ã€‚

ã€é—®é¢˜ã€‘: {question}

è¯·æŒ‰ä»¥ä¸‹ç­–ç•¥è¾“å‡ºï¼š
1. **å¦‚æœæ˜¯äº‹å®ç±»é—®é¢˜** (å¦‚æ—¶é—´ã€åœ°ç‚¹ã€äººç‰©ã€ä½œå“)ï¼š
   - è¯·ç›´æ¥åˆ—å‡ºå…³é”®äº‹å®ï¼ˆKey Factsï¼‰ã€‚
   - ç¤ºä¾‹ï¼š"1994å¹´ä¸Šæ˜ ï¼›å¯¼æ¼”æ˜¯ç½—ä¼¯ç‰¹Â·æ³½ç±³å‰æ–¯ã€‚"
2. **å¦‚æœæ˜¯è§£é‡Š/è§‚ç‚¹ç±»é—®é¢˜** (å¦‚ä¸ºä»€ä¹ˆã€å¦‚ä½•è¯„ä»·ã€åŸç†è§£æ)ï¼š
   - è¯·ç®€è¿°æ ¸å¿ƒæ¦‚å¿µæˆ–å…¬è®¤çš„ä¸»æµè§‚ç‚¹ã€‚
   - ç¤ºä¾‹ï¼š"å› ä¸ºç‘åˆ©æ•£å°„ï¼ˆRayleigh scatteringï¼‰ï¼ŒçŸ­æ³¢é•¿çš„è“å…‰æ›´å®¹æ˜“è¢«æ•£å°„..."

(ä¿æŒå®¢è§‚ï¼Œä¸è¦è‡†é€ ï¼Œå¦‚æœä¸çŸ¥é“å°±è¯´ä¸çŸ¥é“)
"""

FORENSIC_AUDIT_PROMPT = """ä½ æ˜¯ä¸€åé“é¢æ— ç§çš„"å¹»è§‰å®¡åˆ¤å®˜"ã€‚
ä»»åŠ¡: åˆ¤æ–­ã€æ¨¡å‹å›ç­”ã€‘ä¸­æ˜¯å¦åŒ…å«**ç¼–é€ çš„å®ä½“**ã€**ç¯¡æ”¹çš„äº‹å®**æˆ–**é”™è¯¯çš„é€»è¾‘**ã€‚

ä¸ºäº†è¾…åŠ©ä½ çš„åˆ¤æ–­ï¼Œæˆ‘è®©ä½ å…ˆåœ¨åå°å›å¿†äº†ç›¸å…³çš„æ­£ç¡®çŸ¥è¯†ï¼ˆè§ã€å†…éƒ¨è®°å¿†å¿«ç…§ã€‘ï¼‰ï¼Œè¯·å‚è€ƒå®ƒï¼Œä½†æ ¸å¿ƒæ˜¯è¿›è¡Œ**å¯ä¿¡åº¦å®¡è®¡**ã€‚

ã€é—®é¢˜ã€‘: {question}

ã€å†…éƒ¨è®°å¿†å¿«ç…§ (ç”¨äºè¾…åŠ©æ ¸å®)ã€‘: 
{internal_knowledge}

ã€æ¨¡å‹å›ç­” (è¢«å‘Š)ã€‘: 
{answer}

ã€ğŸ” ç¦»çº¿éªŒè¯æŠ¥å‘Šã€‘
{offline_validation_report}

è¯·æ‰§è¡Œä»¥ä¸‹**ä¸‰æ­¥èµ°**å®¡è®¡ç¨‹åºï¼š

1. **åœºæ™¯å®šæ€§ (Scenario Check)**:
   - è¿™æ˜¯ä¸€ä¸ªéœ€è¦åˆ›é€ åŠ›çš„ä»»åŠ¡(å†™æ•…äº‹/ä»£ç )å—ï¼Ÿ -> å¦‚æœæ˜¯ï¼Œä¸”é€»è¾‘é€šé¡ºï¼Œç›´æ¥ PASSã€‚
   - è¿™æ˜¯ä¸€ä¸ªä¸¥è‚ƒçš„äº‹å®é—®ç­”å—ï¼Ÿ -> ç»§ç»­ä¸‹ä¸€æ­¥ã€‚

2. **å®ä½“ä¸ç»†èŠ‚å®¡è®¡ (Entity & Fact Audit)** - *æ ¸å¿ƒç¯èŠ‚*:
   - **æ‰«æ**: æ‰¾å‡ºå›ç­”ä¸­æ‰€æœ‰**å…·ä½“çš„å®ä½“**(äººåã€ä½œå“åã€å¹´ä»½ã€åœ°ç‚¹)ã€‚
   - **è‡ªæˆ‘è´¨è¯¢ (Self-Inquiry)**: 
     * ç»“åˆã€å†…éƒ¨è®°å¿†å¿«ç…§ã€‘é—®è‡ªå·±ï¼š"è¿™ä¸ªç»†èŠ‚(å¦‚1995å¹´)ä¸æˆ‘è®°å¿†ä¸­çš„äº‹å®(å¦‚1994å¹´)å†²çªå—ï¼Ÿ"
     * ç»“åˆã€ç¦»çº¿æŠ¥å‘Šã€‘é—®è‡ªå·±ï¼š"è¿™ä¸ªç”Ÿåƒ»è¯(å¦‚'çˆ¬è¡Œå«çŸ›')æ˜¯çœŸå®å­˜åœ¨çš„ï¼Œè¿˜æ˜¯æ¨¡å‹åœ¨'ä¸€æœ¬æ­£ç»åœ°èƒ¡è¯´å…«é“'ï¼Ÿ"
   - **åˆ¤å®š**: åªè¦å‘ç°ä¸€ä¸ª**ç¡®å‡¿çš„**äº‹å®å†²çªæˆ–ç¼–é€ å®ä½“ -> æ ‡è®°ä¸º **FAIL**ã€‚

3. **é€»è¾‘ä¸æŒ‡ä»¤å®¡æŸ¥ (Logic Audit)**:
   - æ˜¯å¦å­˜åœ¨è‡ªç›¸çŸ›ç›¾ï¼ˆå‰è¨€ä¸æ­åè¯­ï¼‰ï¼Ÿ
   - æ˜¯å¦å­˜åœ¨è¿‡åº¦å…·ä½“åŒ–ï¼ˆåœ¨æ¨¡ç³Šé—®é¢˜ä¸‹ç¼–é€ ç²¾ç¡®åˆ°å°æ•°ç‚¹çš„è™šå‡æ•°æ®ï¼‰ï¼Ÿ
   - æ˜¯å¦ä¸»è¯­å·æ¢ï¼ˆé—®"å¼ ä¸‰"ï¼Œç­”"æå››çš„äº‹è¿¹"ï¼‰ï¼Ÿ

**æœ€ç»ˆåˆ¤å†³æ ‡å‡† (Verdict Criteria)**:
- **FAIL**: 
  - åŒ…å«**ç¡®å‡¿çš„**äº‹å®é”™è¯¯ï¼ˆå¦‚å¹´ä»½é”™ã€å¼ å† ææˆ´ï¼‰ã€‚
  - åŒ…å«æå¤§æ¦‚ç‡æ˜¯**ç¼–é€ çš„**ç»†èŠ‚ï¼ˆç¦»çº¿å·¥å…·æŠ¥è­¦+å†…éƒ¨è®°å¿†æ— æ­¤è®°å½•ï¼‰ã€‚
  - é€»è¾‘å´©åæˆ–ç­”éæ‰€é—®ã€‚
- **PASS**: 
  - æ‰€æœ‰å…³é”®äº‹å®ï¼ˆæ—¶é—´ã€åœ°ç‚¹ã€äººç‰©ï¼‰å‡æ ¸å®æ— è¯¯ã€‚
  - æˆ–è€…æ˜¯åˆ›ä½œç±»ä»»åŠ¡ä¸”é€»è¾‘è‡ªæ´½ã€‚
  - æˆ–è€…ä»…æ˜¯è¡¨è¿°æ–¹å¼ä¸åŒï¼ˆ"é¦–æ˜ äº94å¹´" vs "1994å¹´"ï¼‰ï¼Œä½†æ„æ€å®Œå…¨ä¸€è‡´ã€‚

è¯·è¾“å‡º JSON:
{{
    "step_1_scenario": "...",
    "step_2_entity_audit": "åˆ†æå…³é”®å®ä½“ï¼š[å®ä½“A] -> [æ ¸å®ç»“æœ]...",
    "step_3_logic_audit": "...",
    "status": "PASS" | "FAIL", 
    "reason": "ä¸€é’ˆè§è¡€åœ°æŒ‡å‡ºå“ªé‡Œé”™äº†ï¼Œæˆ–è€…ä¸ºä»€ä¹ˆé€šè¿‡"
}}
"""

CODE_AUDIT_PROMPT = """è§’è‰²:ä»£ç æ­£ç¡®æ€§ä¸æŒ‡ä»¤éµå¾ªå®¡æŸ¥å‘˜ã€‚

ä½ éœ€è¦åˆ¤æ–­åŠ©æ‰‹çš„å›ç­”(åŒ…å«ä»£ç )æ˜¯å¦:
1. **æ»¡è¶³éœ€æ±‚**: å®ç°äº†ç”¨æˆ·è¯·æ±‚çš„åŠŸèƒ½ã€‚
2. **æ— è‡´å‘½é”™è¯¯**: æ²¡æœ‰æ˜æ˜¾çš„è¯­æ³•é”™è¯¯ã€è™šæ„çš„åº“/å‡½æ•°ã€æœªå®šä¹‰çš„ç¬¦å·ã€‚
3. **æ— è™šå‡æ‰§è¡Œ**: æ²¡æœ‰è°ç§°"æˆ‘å·²ç»è¿è¡Œäº†ä»£ç "æˆ–"è¯»å–äº†ä½ çš„æœ¬åœ°æ–‡ä»¶"ã€‚

ã€é—®é¢˜ã€‘: {question}
ã€å›ç­”ã€‘: {answer}

è¯·åˆ¤å®šä»¥ä¸‹å¸ƒå°”å€¼:
- meets_requirement: ä»£ç æ˜¯å¦å®ç°äº†ç”¨æˆ·è¯·æ±‚?
- fatal_error: æ˜¯å¦å­˜åœ¨è‡´å‘½é˜»æ–­æ€§é”™è¯¯?
- misleading_execution_claim: æ˜¯å¦å­˜åœ¨è™šå‡çš„æ‰§è¡Œ/è®¿é—®å£°æ˜?

è¾“å‡º JSON:
{{
  "meets_requirement": true | false,
  "fatal_error": true | false,
  "misleading_execution_claim": true | false,
  "reasons": ["ç®€çŸ­ç†ç”±1", "ç®€çŸ­ç†ç”±2"],
  "evidence": ["å¼•ç”¨å›ç­”ä¸­çš„å…³é”®ç‰‡æ®µ"]
}}
"""

CREATIVE_AUDIT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªåˆ›æ„å†™ä½œå®¡é˜…å‘˜ã€‚
ä»»åŠ¡ï¼šæ£€æŸ¥ã€å¼€æ”¾å¼åˆ›ä½œã€‘çš„é€»è¾‘ä¸€è‡´æ€§ã€‚

ã€å®¡æŸ¥åŸåˆ™ã€‘
1. **å…è®¸è™šæ„**ï¼šå…è®¸å¼•å…¥æ–°ç»†èŠ‚ã€æ–°è®¾å®šï¼Œåªè¦ç¬¦åˆå‰æ–‡é€»è¾‘ã€‚
2. **ç¦æ­¢å†²çª**ï¼šç¦æ­¢ä¸ç”¨æˆ·ç»™å®šçš„å‰æï¼ˆèƒŒæ™¯ï¼‰å‘ç”Ÿç›´æ¥äº‹å®å†²çªã€‚
3. **ç¦æ­¢æ‹’ç­”**ï¼šæ¨¡å‹ä¸åº”æ‹’ç»ç”¨æˆ·çš„åˆ›ä½œè¯·æ±‚ã€‚

ã€ç”¨æˆ·è¾“å…¥ã€‘: {question}
ã€æ¨¡å‹åˆ›ä½œã€‘: {answer}

è¯·è¾“å‡º JSON: {{"analysis": "...", "status": "PASS" | "FAIL", "reason": "..."}}
"""

# ============= ç¦»çº¿éªŒè¯å™¨å‡½æ•°é›† =============

def validate_with_bloom(text: str) -> List[Dict[str, Any]]:
    """
    [ä¿®æ­£ç‰ˆ] ä½¿ç”¨ Bloom Filter æ£€æµ‹ä¸å­˜åœ¨çš„å®ä½“ (æ”¯æŒä¸­è‹±åŒè¯­)
    """
    if not _ensure_bloom_loaded():
        return []
    
    findings = []
    
    # ç­–ç•¥: æ”¶é›†éœ€è¦è¿è¡Œçš„ SpaCy æ¨¡å‹
    docs_to_process = []
    if AppConfig.ENABLE_SPACY_NER:
        # å¦‚æœè‹±æ–‡æ¨¡å‹å¯ç”¨ï¼Œè·‘ä¸€é
        if nlp_en: 
            try:
                docs_to_process.append(nlp_en(text))
            except Exception:
                pass # å¿½ç•¥æ¨¡å‹æŠ¥é”™
        
        # ã€æ ¸å¿ƒä¿®å¤ã€‘å¦‚æœä¸­æ–‡æ¨¡å‹å¯ç”¨ï¼Œä¹Ÿè·‘ä¸€éï¼
        if nlp_zh: 
            try:
                docs_to_process.append(nlp_zh(text))
            except Exception:
                pass
    
    seen_entities = set() # ç”¨äºå»é‡ï¼Œé˜²æ­¢åŒä¸€ä¸ªå®ä½“è¢«ä¸¤ä¸ªæ¨¡å‹éƒ½æŠ“å‡ºæ¥æŠ¥é”™ä¸¤æ¬¡

    for doc in docs_to_process:
        # æå–æ ¸å¿ƒå®ä½“ç±»å‹
        entities = [(ent.text, ent.label_) for ent in doc.ents 
                   if ent.label_ in {"PERSON", "ORG", "GPE", "WORK_OF_ART", "EVENT"}]
        
        for entity_text, entity_type in entities:
            # æ¸…æ´—ä¸€ä¸‹å®ä½“æ–‡æœ¬
            clean_text = entity_text.strip()
            
            # 1. é¿å…é‡å¤å¤„ç†
            if clean_text in seen_entities: 
                continue 
            seen_entities.add(clean_text)
            
            # 2. è¿‡æ»¤æ‰å¤ªçŸ­çš„å®ä½“ (ä¸­æ–‡å•å­—å®¹æ˜“è¯¯æŠ¥ï¼Œå¦‚"æ"ã€"ç‹")
            if len(clean_text) < 2:
                continue

            # 3. Bloom Filter æŸ¥è¯
            # æ³¨æ„ï¼šBloom Filter é‡Œçš„ä¸­æ–‡é€šå¸¸æ˜¯ç®€ä½“ï¼Œä¸éœ€è¦é¢å¤–å¤„ç†ï¼Œç›´æ¥æŸ¥
            if clean_text not in _BLOOM_FILTER:
                findings.append({
                    "type": "unknown_entity",
                    "entity": clean_text,
                    "entity_type": entity_type,
                    "risk": "HIGH",
                    "reason": f"å®ä½“ '{clean_text}' ({entity_type}) ä¸åœ¨çŸ¥è¯†åº“ä¸­ï¼Œå¯èƒ½æ˜¯å¹»è§‰"
                })
    
    return findings


def validate_isbn_doi(text: str) -> List[Dict[str, Any]]:
    """éªŒè¯ ISBN/DOI ç­‰æ ‡å‡†æ ¼å¼"""
    if not HAS_STDNUM:
        return []
    
    findings = []
    
    # ISBN æ£€æµ‹
    isbn_pattern = r'\bISBN[:\s-]*([0-9\-Xx]{10,17})\b'
    for match in re.finditer(isbn_pattern, text, re.IGNORECASE):
        isbn_candidate = match.group(1).replace('-', '').replace(' ', '')
        try:
            stdnum.isbn.validate(isbn_candidate)
        except Exception:
            findings.append({
                "type": "invalid_isbn",
                "value": match.group(0),
                "risk": "HIGH",
                "reason": f"ISBN '{match.group(0)}' æ ¡éªŒä½é”™è¯¯,å¯èƒ½æ˜¯ç¼–é€ çš„"
            })
    
    # DOI æ ¼å¼æ£€æµ‹
    doi_pattern = r'\b(10\.\d{4,}/[^\s]+)'
    for match in re.finditer(doi_pattern, text):
        doi = match.group(1)
        if re.search(r'[<>"\{\}|\\^`\[\]]', doi):
            findings.append({
                "type": "invalid_doi",
                "value": doi,
                "risk": "MEDIUM",
                "reason": f"DOI '{doi}' åŒ…å«éæ³•å­—ç¬¦,æ ¼å¼å¯ç–‘"
            })
    
    return findings


def check_gibberish(text: str) -> Optional[Dict[str, Any]]:
    """æ£€æµ‹æ— æ„ä¹‰æ–‡æœ¬/ä¹±ç """
    if not _ensure_gibberish_loaded():
        return None
    
    try:
        is_gibberish = _GIBBERISH_DETECTOR.is_gibberish(text)
        if is_gibberish:
            return {
                "type": "gibberish_detected",
                "risk": "HIGH",
                "reason": "æ£€æµ‹åˆ°å¤§é‡æ— æ„ä¹‰æ–‡æœ¬æˆ–ä¹±ç ,æ¨¡å‹å¯èƒ½å·²å´©å"
            }
    except Exception as e:
        logger.warning(f"Gibberish detection failed: {e}")
    
    return None


def check_future_dates(text: str) -> List[Dict[str, Any]]:
    """æ£€æµ‹ä¸åˆç†çš„æœªæ¥æ—¥æœŸ"""
    findings = []
    current_year = 2026
    
    year_pattern = r'\b(20[2-9][0-9]|2[1-9][0-9]{2})\b'
    for match in re.finditer(year_pattern, text):
        year = int(match.group(1))
        if year > current_year + 1:
            context_start = max(0, match.start() - 50)
            context_end = min(len(text), match.end() + 50)
            context = text[context_start:context_end].lower()
            
            past_indicators = ["was", "were", "had", "did", "å·²", "æ›¾", "è¿‡å»", "å½“æ—¶"]
            if any(indicator in context for indicator in past_indicators):
                findings.append({
                    "type": "future_date_in_past_context",
                    "value": match.group(0),
                    "risk": "HIGH",
                    "reason": f"å¹´ä»½ {year} æ˜¯æœªæ¥æ—¶é—´,ä½†è¯­å¢ƒæš—ç¤ºå·²å‘ç”Ÿ"
                })
    
    return findings


def run_offline_validation(text: str) -> Dict[str, Any]:
    """è¿è¡Œæ‰€æœ‰ç¦»çº¿éªŒè¯å™¨,è¿”å›æ±‡æ€»æŠ¥å‘Š"""
    all_findings = []
    
    # 1. Gibberish æ£€æµ‹ (æœ€å¿«,ä¼˜å…ˆçº§æœ€é«˜)
    if AppConfig.ENABLE_GIBBERISH_CHECK:
        gibberish_result = check_gibberish(text)
        if gibberish_result:
            return {
                "critical_issue": gibberish_result,
                "findings": [gibberish_result],
                "recommendation": "IMMEDIATE_FAIL"
            }
    
    # 2. Bloom Filter å®ä½“éªŒè¯
    if AppConfig.ENABLE_BLOOM_FILTER:
        bloom_findings = validate_with_bloom(text)
        all_findings.extend(bloom_findings)
    
    # 3. ISBN/DOI æ ¡éªŒ
    if AppConfig.ENABLE_ISBN_CHECK:
        format_findings = validate_isbn_doi(text)
        all_findings.extend(format_findings)
    
    # 4. æœªæ¥æ—¥æœŸæ£€æµ‹
    date_findings = check_future_dates(text)
    all_findings.extend(date_findings)
    
    # æ±‡æ€»é£é™©ç­‰çº§
    high_risk_count = sum(1 for f in all_findings if f.get("risk") == "HIGH")
    
    recommendation = "NO_ISSUE"
    if high_risk_count >= 2:
        recommendation = "STRONG_FAIL"
    elif high_risk_count == 1:
        recommendation = "SUSPICIOUS"
    elif all_findings:
        recommendation = "MINOR_CONCERN"
    
    return {
        "findings": all_findings,
        "high_risk_count": high_risk_count,
        "total_findings": len(all_findings),
        "recommendation": recommendation
    }


def format_offline_validation_report(result: Dict[str, Any]) -> str:
    """å°†ç¦»çº¿éªŒè¯å™¨çš„ç»“æœå­—å…¸è½¬æ¢ä¸º Prompt å¯è¯»çš„æ–‡æœ¬æŠ¥å‘Š"""
    if not result or not result.get("findings"):
        return "ã€ç¦»çº¿éªŒè¯çŠ¶æ€ã€‘: é€šè¿‡ (æœªå‘ç°æ˜æ˜¾å¼‚å¸¸)"
    
    lines = ["ã€ç¦»çº¿éªŒè¯è­¦æŠ¥ã€‘: âš ï¸ å‘ç°æ½œåœ¨é£é™©,è¯·é‡ç‚¹æ’æŸ¥ä»¥ä¸‹é¡¹ç›®:"]
    
    findings = result.get("findings", [])
    for i, f in enumerate(findings, 1):
        f_type = f.get("type", "unknown")
        reason = f.get("reason", "")
        risk = f.get("risk", "LOW")
        
        icon = "âŒ" if risk == "HIGH" else "âš ï¸"
        lines.append(f"{i}. {icon} [{f_type}]: {reason}")
        
    lines.append("\n(è¯·åœ¨ç»ˆå®¡åˆ¤å†³ä¸­å……åˆ†è€ƒè™‘ä¸Šè¿°éªŒè¯ç»“æœ,å¦‚æœå®ä½“ç¡®å®ä¸å­˜åœ¨,è¯·åˆ¤ FAIL)")
    return "\n".join(lines)


def format_nli_evidence_for_prompt(nli_results: List[dict]) -> str:
    """æ ¼å¼åŒ– NLI è¯æ®"""
    if not nli_results:
        return "æ—  NLI é£é™©å¥æ£€æµ‹åˆ°ã€‚"

    lines = []
    for idx, item in enumerate(nli_results):
        sent = item.get("sentence", "").strip()
        prob = item.get("prob", 0.0)
        
        if prob > 0.9:
            risk_level = "é«˜"
        elif prob > 0.5:
            risk_level = "ä¸­"
        else:
            risk_level = "ä½"
        
        lines.append(f"{idx+1}. é£é™©å¥: \"{sent}\" (é£é™©ç­‰çº§: {risk_level})")

    return "\n".join(lines)


# ================= Graph Construction =================

class GraphState(TypedDict):
    question: str
    answer: str
    reference: Optional[str]
    intent: str
    nli: Optional[List[dict]]
    offline_validation: Optional[Dict[str, Any]]
    final_result: dict

parser = RobustJsonParser()


async def intent_node(state: GraphState):
    """
    ä¸‰çº§é˜²å¾¡è·¯ç”±èŠ‚ç‚¹:
    1. ç‰©ç†é”: æœ‰ Ref -> CONSTRAINED
    2. å…³é”®è¯é”: ä»£ç /åˆ›æ„è¯ -> CODE/CREATIVE
    3. LLM è·¯ç”±: å¤„ç†å‰©ä½™æ¨¡ç³Šæƒ…å†µ
    """
    q = state.get("question", "") or ""
    ql = q.lower()
    ref = state.get("reference")

    # æ¸…æ´— ref
    if ref and len(str(ref).strip()) < 5:
        ref = None

    # --- é˜²å¾¡å±‚ 1: ç‰©ç†é” (Reference) ---
    # åªè¦æœ‰èµ„æ–™ï¼Œå¼ºåˆ¶èµ° CONSTRAINED
    if ref:
        return {"intent": "CONSTRAINED"}

    # --- é˜²å¾¡å±‚ 2: å…³é”®è¯é” (Heuristics) ---

    # 2.1 ä»£ç å…³é”®è¯
    code_kws = [
        "code", "function", "script", "program", "sql", "regex", "python", "java",
        "å†™ä»£ç ", "å‡½æ•°", "è„šæœ¬", "ç¼–ç¨‹", "æ­£åˆ™"
    ]
    if any(k in ql for k in code_kws):
        return {"intent": "CODE"}

    # 2.2 åˆ›æ„å…³é”®è¯ (æ–°å¢)
    # è¿™äº›è¯å‡ºç°æ—¶ï¼Œç›´æ¥è·³è¿‡ LLMï¼Œé˜²æ­¢è¢«è¯¯åˆ¤ä¸º QA(Forensic) å¯¼è‡´è¯¯æ€
    creative_kws = [
        "story", "poem", "novel", "fiction", "joke", "script", "act as", "pretend", "imagine",
        "email", "letter", "essay", "lyrics", "parody",
        "æ•…äº‹", "å°è¯´", "è¯—", "ç¬‘è¯", "å‰§æœ¬", "æ‰®æ¼”", "å‡è®¾", "æƒ³è±¡", "ç¼–é€ ", 
        "é‚®ä»¶", "ä¿¡", "ä½œæ–‡", "æ­Œè¯", "ç»­å†™", "æ‰©å†™", "æ‹Ÿäºº"
    ]
    
    # åŠ questionå…³é”®å­—æ’é™¤
    question_indicators = ["ä¸ºä»€ä¹ˆ", "ä¸ºä½•", "what", "why", "how", "who", "å“ªä½", "å“ªä¸ª", "è§£é‡Š", "å«ä¹‰", "æ˜¯å•¥", "æ˜¯ä»€ä¹ˆ", "mean"]
    is_question = any(idx in ql for idx in question_indicators) or "?" in q or "ï¼Ÿ" in q

    if any(k in ql for k in creative_kws) and not is_question:
        return {"intent": "CREATIVE"}

    # --- é˜²å¾¡å±‚ 3: LLM è¯­ä¹‰è£å†³ ---
    try:
        chain = ChatPromptTemplate.from_template(INTENT_PROMPT) | llm | parser
        res = await chain.ainvoke({"question": q})
        intent = str(res.get("type", "QA")).upper()

        # å…œåº• A: LLM è¯¯åˆ¤ CONSTRAINED ä½†æ²¡ Ref -> é™çº§ä¸º QA
        if intent == "CONSTRAINED":
            logger.warning(f"Router predicted CONSTRAINED but no ref. Forcing QA.")
            intent = "QA"

        # å…œåº• B: å½’ä¸€åŒ–
        valid_intents = {"CREATIVE", "CODE", "QA", "CONSTRAINED"}
        if intent not in valid_intents:
            intent = "QA"

        return {"intent": intent}

    except Exception as e:
        logger.warning(f"Intent router error: {e}, defaulting to QA")
        return {"intent": "QA"}


def router_node(state: GraphState):
    intent = state.get("intent", "QA")
    valid_intents = {"CONSTRAINED", "CODE", "QA", "CREATIVE", "GENERAL"}
    if intent not in valid_intents:
        logger.warning(f"Unknown intent: {intent}, defaulting to QA")
        return "QA"
    # å…¼å®¹æ—§ä»£ç : GENERAL -> QA
    if intent == "GENERAL":
        return "QA"
    return intent


def _invoke_with_retry(chain, inputs: dict, retries: int = 1) -> dict:
    """è§£æå¤±è´¥é‡è¯•"""
    last = None
    for _ in range(retries + 1):
        last = chain.invoke(inputs)
        if isinstance(last, dict):
            reason = str(last.get("reason", "")).lower()
            status = str(last.get("status", "")).upper()
            if status == "FAIL" and "parsing error" in reason:
                continue
        break
    return last if isinstance(last, dict) else {"status": "FAIL", "reason": "Chain returned non-dict."}


async def _ainvoke_with_retry(chain, inputs: dict, retries: int = 1) -> dict:
    """å¼‚æ­¥è§£æå¤±è´¥é‡è¯•"""
    last = None
    for _ in range(retries + 1):
        last = await chain.ainvoke(inputs)
        if isinstance(last, dict):
            reason = str(last.get("reason", "")).lower()
            status = str(last.get("status", "")).upper()
            if status == "FAIL" and "parsing error" in reason:
                continue
        break
    return last if isinstance(last, dict) else {"status": "FAIL", "reason": "Chain returned non-dict."}


async def forensic_chk(state: GraphState):
    question = state.get("question", "")
    answer = state.get("answer", "")
    
    # --- Parallel Task Launch ---
    
    # Task A: Start Knowledge Injection (Network I/O)
    # We use a helper coroutine or direct ainvoke to run this concurrently
    async def fetch_knowledge():
        try:
            from langchain_core.messages import HumanMessage
            k_msg = KNOWLEDGE_RETRIEVAL_PROMPT.format(question=question)
            # Use ainvoke for async
            raw_k_res = await llm.ainvoke([HumanMessage(content=k_msg)])
            k_text = raw_k_res.content.strip()
            if len(k_text) > 600:
                k_text = k_text[:600] + "..."
            return k_text
        except Exception as e:
            logger.warning(f"Knowledge injection failed: {e}")
            return "æ— å†…éƒ¨çŸ¥è¯†"

    # Launch Task A
    knowledge_task = asyncio.create_task(fetch_knowledge())

    # Task B: Run Offline Validation (CPU Bound)
    # Since this is local CPU work, it runs immediately on the main thread.
    # While it runs, the LLM request (Task A) is waiting for network response.
    offline_result = run_offline_validation(answer)
    
    # Check for critical offline failure
    if offline_result.get("recommendation") == "IMMEDIATE_FAIL":
        # Cancel the LLM task if we fail early to save tokens/time
        knowledge_task.cancel()
        critical = offline_result.get("critical_issue", {})
        return {"final_result": {
            "status": "FAIL", 
            "risk_level": "HIGH", 
            "reason": f"ç¦»çº¿éªŒè¯ç†”æ–­: {critical.get('reason')}",
            "trace": {"offline": offline_result}
        }}

    # Task A Join: Await the knowledge result
    generated_knowledge = await knowledge_task

    # --- Step 3: Call V5 Audit Prompt ---
    validation_report = format_offline_validation_report(offline_result)
    
    inputs = {
        "question": question,
        "answer": answer,
        "offline_validation_report": validation_report,
        "internal_knowledge": generated_knowledge # ä½œä¸º"è®°å¿†å¿«ç…§"ä¼ å…¥
    }
    
    # ä½¿ç”¨æ›´æ–°åçš„ FORENSIC_AUDIT_PROMPT
    chain = ChatPromptTemplate.from_template(FORENSIC_AUDIT_PROMPT) | llm | parser
    # Use async helper
    raw = await _ainvoke_with_retry(chain, inputs, retries=1)
    
    result = standardize_result(raw)
    
    if "trace" not in result: result["trace"] = {}
    result["trace"]["generated_knowledge"] = generated_knowledge
    result["trace"]["offline_validation"] = offline_result
    
    return {"final_result": result}


# ä¿®æ”¹ enhanced_forensic.py ä¸­çš„ constrained_chk å‡½æ•°

async def constrained_chk(state: GraphState):
    """ä»²è£èŠ‚ç‚¹:æ ¹æ®æ˜¯å¦æœ‰ MiniCheck è¯æ®,åŠ¨æ€åˆ†æµ Prompt"""
    inputs = {
        "question": state["question"], 
        "answer": state["answer"], 
        "reference": state.get("reference", "N/A")
    }
    
    evidence_list = state.get("nli", [])
    
    # ================= [æ–°å¢] åŠ¨æ€åˆ¤ä¾‹æ£€ç´¢é€»è¾‘ =================
    few_shot_text = "(æš‚æ— ç›¸å…³åˆ¤ä¾‹ï¼Œè¯·ä¸¥æ ¼åŸºäºé“å¾‹åˆ¤ç½š)"
    
    # åªæœ‰å½“ NLI æŠ¥è­¦äº†ï¼Œæˆ–è€…ä½ å¸Œæœ›å…¨é‡éƒ½åŠ åˆ¤ä¾‹æ—¶æ‰æ£€ç´¢
    # è¿™é‡Œå»ºè®®: åªè¦å¼€å¯äº† ENABLE_DYNAMIC_FEW_SHOT å°±æ£€ç´¢
    if AppConfig.ENABLE_DYNAMIC_FEW_SHOT and _ensure_selector_loaded():
        try:
            # æ£€ç´¢ 2 ä¸ªæœ€ç›¸ä¼¼çš„ä¾‹å­
            # ä¼ å…¥ Ref, Q, A ç¡®ä¿å…¨æ–¹ä½åŒ¹é…
            # Note: _FEW_SHOT_SELECTOR.retrieve is synchronous (local embedding). 
            # In a true high-concurrency setup, we might wrap this in loop.run_in_executor, 
            # but for now, it's fast enough or acceptable to block briefly.
            examples = _FEW_SHOT_SELECTOR.retrieve(
                inputs['reference'], 
                inputs['question'], 
                inputs['answer'], 
                k=2
            )
            
            # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„ä¾‹å­
            formatted_exs = []
            for i, ex in enumerate(examples, 1):
                # è¯»å–ä½ çš„ JSON å­—æ®µ
                # æ³¨æ„: ä½ çš„ json ç”¨çš„æ˜¯ 'label' è€Œä¸æ˜¯ 'status'
                label = ex.get('label', 'UNKNOWN') 
                reason = ex.get('reason', 'æ— ')
                # æˆªå–ä¸€éƒ¨åˆ†å†…å®¹å±•ç¤ºï¼Œé˜²æ­¢ prompt è¿‡é•¿
                ref_text = ex.get('reference', '')[:150] + "..."
                q_text = ex.get('question', '')
                a_text = ex.get('answer', '')
                
                formatted_exs.append(
                    f"**æ¡ˆä¾‹ {i} [{label}]**:\n"
                    f"- å‚è€ƒ: {ref_text}\n"
                    f"- é—®é¢˜: {q_text}\n"
                    f"- å›ç­”: {a_text}\n"
                    f"- åˆ¤å†³: {label}\n"
                    f"- ç†ç”±: {reason}"
                )
            
            few_shot_text = "\n\n".join(formatted_exs)
            logger.info(f"å·²æ³¨å…¥ {len(examples)} æ¡åŠ¨æ€åˆ¤ä¾‹")
            
        except Exception as e:
            logger.error(f"Dynamic Few-Shot æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            
    # ================= End of Dynamic Logic =================

    if not evidence_list:
        logger.info("MiniCheck passed or disabled. Using STANDARD prompt.")
        # å¦‚æœä½ æƒ³ç»™æ™®é€šæ¨¡å¼ä¹ŸåŠ  Few-Shotï¼Œå¯ä»¥æŠŠ few_shot_examples å¡è¿› CONSTRAINED_AUDIT_PROMPT
        # ä½†ç›®å‰æˆ‘ä»¬åªæ”¹äº† AUGMENTED ç‰ˆæœ¬
        prompt_template = ChatPromptTemplate.from_template(CONSTRAINED_AUDIT_PROMPT)
    else:
        logger.info(f"MiniCheck risks found ({len(evidence_list)}). Using AUGMENTED prompt.")
        evidence_str = format_nli_evidence_for_prompt(evidence_list)
        
        inputs["nli_evidence"] = evidence_str
        # ã€å…³é”®ã€‘æ³¨å…¥ few_shot_examples
        inputs["few_shot_examples"] = few_shot_text
        
        prompt_template = ChatPromptTemplate.from_template(AUGMENTED_CONSTRAINED_PROMPT)
    
    chain = prompt_template | llm | parser
    raw = await _ainvoke_with_retry(chain, inputs, retries=1)
    
    return {"final_result": standardize_result(raw)}


def _to_bool(x):
    if isinstance(x, bool):
        return x
    if x is None:
        return None
    if isinstance(x, (int, float)):
        return bool(x)
    if isinstance(x, str):
        s = x.strip().lower()
        if s in {"true", "yes", "y", "1"}:
            return True
        if s in {"false", "no", "n", "0"}:
            return False
    return None


def _finalize_code_audit(raw: dict) -> dict:
    if not isinstance(raw, dict):
        return {"status": "FAIL", "risk_level": "LOW", "reason": "Code audit returned non-dict."}

    d = {k.lower(): v for k, v in raw.items()}
    meets = _to_bool(d.get("meets_requirement"))
    fatal = _to_bool(d.get("fatal_error"))
    mislead = _to_bool(d.get("misleading_execution_claim"))

    if meets is None and fatal is None and mislead is None:
        return raw

    is_fail = (fatal is True) or (mislead is True) or (meets is False)
    raw["status"] = "FAIL" if is_fail else "PASS"

    if "risk_level" not in d:
        if is_fail and (fatal is True or mislead is True):
            raw["risk_level"] = "HIGH"
        elif is_fail:
            raw["risk_level"] = "MEDIUM"
        else:
            raw["risk_level"] = "LOW"

    if "reason" not in d:
        reasons = d.get("reasons")
        if isinstance(reasons, list) and reasons:
            raw["reason"] = "; ".join(str(r) for r in reasons[:3])
        else:
            if fatal is True:
                raw["reason"] = "Code contains an obvious fatal issue."
            elif mislead is True:
                raw["reason"] = "Misleading execution/access claim detected."
            elif meets is False:
                raw["reason"] = "Code does not meet the user's requirement."
            else:
                raw["reason"] = "Code audit passed."

    return raw


async def code_chk(state: GraphState):
    chain = ChatPromptTemplate.from_template(CODE_AUDIT_PROMPT) | llm | parser
    raw = await _ainvoke_with_retry(chain, state, retries=1)
    raw = _finalize_code_audit(raw)
    return {"final_result": standardize_result(raw)}


async def creative_chk(state: GraphState):
    """åˆ›æ„ç±»ä¸æŸ¥ Bloom Filterï¼ŒåªæŸ¥é€»è¾‘è‡ªæ´½"""
    chain = ChatPromptTemplate.from_template(CREATIVE_AUDIT_PROMPT) | llm | parser
    raw = await _ainvoke_with_retry(chain, state, retries=1)
    return {"final_result": standardize_result(raw)}


# ================= NLI Sentence Probe =================

def _split_sentences_mixed(text: str) -> List[str]:
    """
    [å‡çº§ç‰ˆ] ä¼˜å…ˆä½¿ç”¨ SpaCy è¿›è¡Œè¯­ä¹‰åˆ†å¥ï¼Œå›é€€åˆ°æ­£åˆ™
    """
    if not text:
        return []
    t = text.strip()
    
    # 1. å°è¯•ä½¿ç”¨ SpaCy (ä¸­è‹±æ–‡å…¼å®¹)
    # è‹±æ–‡ä¼˜å…ˆ
    if nlp_en:
        try:
            doc = nlp_en(t)
            # è¿‡æ»¤æ‰è¿‡çŸ­çš„ç¢ç‰‡ (æ¯”å¦‚ "Yes.")ï¼Œé¿å… NLI è¯¯åˆ¤ï¼Œä½†ä¿ç•™ meaningful çš„çŸ­å¥
            return [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 3]
        except Exception as e:
            logger.warning(f"SpaCy EN split failed: {e}")

    # ä¸­æ–‡å¤‡é€‰
    if nlp_zh:
        try:
            doc = nlp_zh(t)
            return [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 1]
        except Exception:
            pass

    # 2. æ­£åˆ™å…œåº• (ä¿æŒåŸæœ‰é€»è¾‘ä½œä¸º Fallback)
    # ä¿®å¤: é¿å…æŠŠ "Reference 1." è¿™ç§å¼•ç”¨åˆ‡æ–­
    t = re.sub(r'(Ref|Fig|No|Vol)\.\s+', r'\1<DOT> ', t)
    
    parts = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+', t)
    
    out = []
    for p in parts:
        p = p.replace("<DOT>", ".").strip()
        if len(p) > 5: # ç¨å¾®æé«˜ä¸€ç‚¹æœ€å°é•¿åº¦é˜ˆå€¼
            out.append(p)
            
    return out


def _is_meta_sentence(text: str) -> bool:
    """ç²¾ç»†åŒ–å…ƒå¥è¿‡æ»¤"""
    t = text.strip().lower()
    
    meta_phrases = [
        "based on the provided", "based on the given", "according to the passage",
        "answer to the question", "context provided", "given passages",
        "per passage", "refer to passage", "see passage", "described in passage",
        "reference passage", "from passage", "in passage",
        "æ ¹æ®æä¾›çš„", "æ ¹æ®ä¸Šæ–‡", "åŸºäºå‚è€ƒèµ„æ–™", "ä½œä¸ºä¸€ä¸ªai", "è¯­è¨€æ¨¡å‹",
    ]
    if any(p in t for p in meta_phrases):
        return True

    if re.match(r'^[\(\[].{0,40}[\)\]][\.\s]*', t):
        return True
        
    if re.match(r'^(per\s+)?(passage|source|reference)\s+\d+(?:\s+(and|&)\s+\d+)?[\.:\s]*', t):
        return True
    
    return False


async def minicheck_node(state: GraphState):
    """
    [å‡çº§ç‰ˆ] åŒé˜ˆå€¼æ¼æ–—æœºåˆ¶ (Dual-Threshold Funnel)
    ç­–ç•¥:
    1. Score > 0.7 -> FAST FAIL (é«˜ç²¾åº¦æ‹¦æˆªï¼Œé›¶ LLM å¼€é”€)
    2. Score > 0.02 -> NEEDS AUDIT (é«˜å¬å›ç­›é€‰ï¼Œäº¤ç»™ LLM è¾©æŠ¤)
    3. Score <= 0.02 -> SAFE (ç›´æ¥æ”¾è¡Œ)
    """
    ref = state.get("reference", "")
    ans = state.get("answer", "")
    
    # å¦‚æœæ²¡æœ‰ Referenceï¼Œæˆ–è€… MiniCheck æœªåŠ è½½ï¼Œç›´æ¥è·³è¿‡ (è§†ä¸ºæ— é£é™©)
    if not ref or len(str(ref)) < 10 or not _ensure_minicheck_loaded(): 
        return {"nli_verdict": "SAFE", "nli": []}
    
    # 1. åˆ†å¥
    valid_sents = [s for s in _split_sentences_mixed(ans) if not _is_meta_sentence(s)]
    if not valid_sents: 
        return {"nli_verdict": "SAFE", "nli": []}

    try:
        # 2. æ‰¹é‡æ¨ç†
        docs = [ref] * len(valid_sents)
        # Note: In a fully async pipeline, we could offload this to a thread,
        # but for now we keep it blocking as it's the primary task of this node.
        pred_label, probs, _, _ = _MINICHECK_SCORER.score(docs=docs, claims=valid_sents)
        
        # 3. æ”¶é›†è¯æ®ä¸æœ€å¤§é£é™©
        evidence = []
        max_hallu_score = 0.0
        
        for i, prob in enumerate(probs):
            # å¤„ç† list/float æ ¼å¼å·®å¼‚
            prob_val = float(prob[0]) if isinstance(prob, (list, tuple, np.ndarray)) else float(prob)
            
            hallu_score = 1.0 - prob_val # å¹»è§‰æ¦‚ç‡
            
            if hallu_score > max_hallu_score:
                max_hallu_score = hallu_score
            
            # åªè¦è¿›å…¥é»„è‰²é¢„è­¦åŒº (>0.02)ï¼Œå°±è®°å½•æ¡ˆåº•ï¼Œä¾› LLM å‚è€ƒ
            if hallu_score > 0.02:
                evidence.append({
                    "sentence": valid_sents[i],
                    "prob": hallu_score,
                    "risk": "HIGH" if hallu_score > 0.6 else "MEDIUM"
                })

        logger.info(f"ğŸ” MiniCheck Max Risk: {max_hallu_score:.4f} | Evidence Count: {len(evidence)}")

        # === 4. æ¼æ–—åˆ†æµé€»è¾‘ (æ ¸å¿ƒä¿®æ”¹) ===

        # ã€çº¢è‰²åŒºé—´ã€‘: é“è¯å¦‚å±± -> ç›´æ¥åˆ¤æ­»åˆ‘
        if max_hallu_score > 0.7:
            logger.info("ğŸ›‘ Fast-Fail triggered! Skipping LLM.")
            return {
                "nli_verdict": "FAST_FAIL",
                "nli": evidence,
                # ç›´æ¥ç”Ÿæˆ Final Result
                "final_result": {
                    "status": "FAIL",
                    "risk_level": "HIGH",
                    "reason": f"NLIæ¨¡å‹æ£€æµ‹åˆ°ç¡®å‡¿çš„å¹»è§‰ (ç½®ä¿¡åº¦ {max_hallu_score:.2f})ï¼Œè§¦å‘å¿«é€Ÿæ‹¦æˆªã€‚",
                    "trace": {"nli_score": max_hallu_score}
                }
            }
            
        # ã€é»„è‰²åŒºé—´ã€‘: ç–‘ç½ªä»æœ‰ -> äº¤ç»™ LLM å¬è¯
        elif max_hallu_score > 0.02:
            return {
                "nli_verdict": "NEEDS_AUDIT",
                "nli": evidence
            }
            
        # ã€ç»¿è‰²åŒºé—´ã€‘: å®‰å…¨ -> ç›´æ¥é€šè¿‡
        else:
            return {
                "nli_verdict": "SAFE",
                "nli": [],
                # ç›´æ¥ç”Ÿæˆ Final Result (Pass)
                "final_result": {
                    "status": "PASS",
                    "risk_level": "LOW",
                    "reason": "ç»NLIæ¨¡å‹äº¤å‰éªŒè¯ï¼Œæœªå‘ç°æ˜æ˜¾äº‹å®å†²çªã€‚",
                    "trace": {"nli_score": max_hallu_score}
                }
            }
        
    except Exception as e:
        logger.error(f"MiniCheck Error: {e}", exc_info=True)
        # å‡ºé”™æ—¶é»˜è®¤å›é€€åˆ° LLM æ£€æŸ¥ï¼Œä¿è¯å®‰å…¨æ€§
        return {"nli_verdict": "NEEDS_AUDIT", "nli": []}

# ================= MiniCheck Router å‡½æ•° =================
def minicheck_router(state: GraphState):
    verdict = state.get("nli_verdict", "NEEDS_AUDIT")
    
    if verdict == "NEEDS_AUDIT":
        # åªæœ‰å­˜ç–‘æ—¶ï¼Œæ‰å»è·‘ LLM (constrained_chk)
        return "constrained_chk"
    else:
        # FAST_FAIL æˆ– SAFE éƒ½å·²ç»ç”Ÿæˆäº† final_resultï¼Œç›´æ¥ç»“æŸ
        return END


# ================= Workflow =================

workflow = StateGraph(GraphState)
workflow.add_node("intent_classifier", intent_node)
workflow.add_node("constrained_chk", constrained_chk)
workflow.add_node("forensic_chk", forensic_chk)
workflow.add_node("code_chk", code_chk)
workflow.add_node("creative_chk", creative_chk)
workflow.add_node("minicheck_node", minicheck_node)

workflow.set_entry_point("intent_classifier")

# å…¥å£è·¯ç”±
workflow.add_conditional_edges(
    "intent_classifier",
    router_node,
    {
        "CONSTRAINED": "minicheck_node", # 1. æœ‰ Reference -> è¿› NLI
        "CODE": "code_chk",
        "QA": "forensic_chk",
        "CREATIVE": "creative_chk",
        "GENERAL": "forensic_chk"
    },
)

# ã€ä¿®æ”¹ç‚¹ã€‘NLI èŠ‚ç‚¹çš„æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    "minicheck_node",
    minicheck_router,
    {
        "constrained_chk": "constrained_chk", # å­˜ç–‘ -> LLM
        END: END                              # ç¡®ä¿¡ -> ç»“æŸ
    }
)

# å…¶ä»–èŠ‚ç‚¹ç›´æ¥ç»“æŸ
workflow.add_edge("constrained_chk", END)
workflow.add_edge("code_chk", END)
workflow.add_edge("forensic_chk", END)
workflow.add_edge("creative_chk", END)

app = workflow.compile()


# ================= API Wrapper =================

class RemoteHTTPGen:
    def __init__(self, host, model_name, timeout=120.0, api_key=None):
        global llm
        base_url = f"http://{host}/v1" if not host.startswith("http") else f"{host}/v1"
        AppConfig.API_BASE = base_url
        AppConfig.MODEL_NAME = model_name
        llm = ChatOpenAI(
            base_url=base_url,
            api_key=api_key or "sk-x",
            model=model_name,
            temperature=AppConfig.TEMPERATURE,
            timeout=timeout,
        )


def run_fast_validation(gen, question: str, raw_answer: str, **kwargs) -> Dict[str, Any]:
    """ä¸»å…¥å£å‡½æ•°:è¿è¡Œå®Œæ•´çš„éªŒè¯æµç¨‹"""
    global llm
    if llm is None:
        llm = ChatOpenAI(
            base_url=AppConfig.API_BASE,
            api_key=AppConfig.API_KEY or "sk-x",
            model=AppConfig.MODEL_NAME,
            temperature=AppConfig.TEMPERATURE,
        )

    # Step 1: refusal æ£€æµ‹/æ¸…æ´—
    pre_result, final_answer = clean_and_check_refusal(
        question, raw_answer, AppConfig.STRICT_REFUSAL_CHECK
    )
    if pre_result:
        out = standardize_result(pre_result)
        out["trace"] = out.get("trace", {})
        out["trace"]["intent"] = "PRE_REFUSAL_BLOCK"
        return out

    # Step 2: å‡†å¤‡è¾“å…¥(æå– reference)
    ref_input = kwargs.get("reference") or kwargs.get("knowledge")
    final_q, extracted_ref = smart_extract(question)
    final_ref = ref_input if ref_input else extracted_ref

    # Step 3: LLM/NLI å›¾å®¡è®¡ (Async Execution)
    try:
        # ä½¿ç”¨ asyncio.run è¿è¡Œå¼‚æ­¥å›¾
        state = asyncio.run(app.ainvoke({
            "question": final_q, 
            "answer": final_answer, 
            "reference": final_ref
        }))
        
        final = state.get("final_result", {
            "status": "FAIL", 
            "risk_level": "LOW", 
            "reason": "Graph execution error"
        })
        
        if "trace" not in final or not isinstance(final["trace"], dict):
            final["trace"] = {}
        final["trace"]["intent"] = state.get("intent")
        if state.get("nli") is not None:
            final["trace"]["nli_detections"] = len(state.get("nli", []))
            final["trace"]["nli_details"] = state.get("nli", [])
        
        return final
        
    except Exception as e:
        logger.error(f"Graph execution error: {e}", exc_info=True)
        return {
            "status": "FAIL", 
            "risk_level": "LOW", 
            "reason": f"Graph execution failed: {str(e)}", 
            "trace": {"intent": "GRAPH_ERROR", "error": str(e)}
        }